{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30014e22",
   "metadata": {},
   "source": [
    "# Spatial synapse analyses\n",
    "This notebook is for testing out a few ways to quantify the spatial clustering of synapses from the Hemibrain connectome. It was originally drafted in my oviIN_analyses_gabrielle repo, but I copied it over here and will add to it or modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7755158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56cbda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuprint import Client\n",
    "# remove my token before making notebook public\n",
    "c = Client('neuprint.janelia.org', dataset='hemibrain:v1.2.1', token='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6ImdnMjExNEBjb2x1bWJpYS5lZHUiLCJsZXZlbCI6Im5vYXV0aCIsImltYWdlLXVybCI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hLS9BT2gxNEdpb1lJLUVPLWdidGxPRTh6SmQ0eF9ZQ1Y4ZHF0YVFjWGlHeG5CMz1zOTYtYz9zej01MD9zej01MCIsImV4cCI6MTgxMDUyOTYzNH0.jv9eR0SH5RhfBdXrtp4r-dDFOhcsT8GBbE4v69ysCKs') \n",
    "c.fetch_version()\n",
    "\n",
    "# import important stuff here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuprint import fetch_simple_connections, fetch_synapse_connections, NeuronCriteria as NC, fetch_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f900c",
   "metadata": {},
   "source": [
    "## Ripley K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3ade0",
   "metadata": {},
   "source": [
    "### pypi Ripley K\n",
    "Testing out ripleyk package using their example: https://pypi.org/project/ripleyk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "xs = []\n",
    "ys = []\n",
    "zs = []\n",
    "radius = 1\n",
    "random.seed(0)\n",
    "for i in range(0,10000):\n",
    "    positioned = False\n",
    "    while positioned is False:\n",
    "        x = random.uniform(-radius, radius)\n",
    "        y = random.uniform(-radius, radius)\n",
    "        z = random.uniform(-radius, radius)\n",
    "        if (x**2)+(y**2)+(z**2) < radius**2:\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "            zs.append(z)\n",
    "            positioned = True\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "zs = np.array(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ripleyk\n",
    "\n",
    "radius = 0.5\n",
    "bounding_radius = 1\n",
    "k = ripleyk.calculate_ripley(radius, bounding_radius, d1=xs, d2=ys)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc37bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.pi * (radius ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad3c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the cloud of random points\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(xs, ys, zs)\n",
    "\n",
    "# superimpose a sphere of bounding radius using the bounding_radius variable defined above\n",
    "\n",
    "\n",
    "#u = np.linspace(0, 2 * np.pi, 100)\n",
    "#v = np.linspace(0, np.pi, 100)\n",
    "#x = bounding_radius * np.outer(np.cos(u), np.sin(v))\n",
    "#y = bounding_radius * np.outer(np.sin(u), np.sin(v))\n",
    "#z = bounding_radius * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "#ax.plot_wireframe(x, y, z, color='r', alpha=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803e975",
   "metadata": {},
   "source": [
    "I thought this would actually be really simple but I'm a little stumped. First, if the points are from a uniform distribution, I expect them to be homogenous which should give a k equal to pi * r^2. That is approximately what I get if r = 0.5, but why is it consistently off by 0.03? Relatedly, the inputs to this function are not well-explained in the documentation. Which of the inputs is the search radius and which is the size of the sample region? The equations look slightly differently from those in the Wikipedia page (https://en.wikipedia.org/wiki/Spatial_descriptive_statistics#Ripley's_K_and_L_functions). \n",
    "\n",
    "This is the kind of function that is simple enough that I can write it myself if this existing function/package is too ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a797c1",
   "metadata": {},
   "source": [
    "### squidpy\n",
    "I'll give this one a try because the documentation is better. The hardest part is getting the input data into a very specific format. This package is meant for molecular bio data. \n",
    "https://squidpy.readthedocs.io/en/stable/api/squidpy.gr.ripley.html#squidpy.gr.ripley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(42)\n",
    "counts = rng.integers(0, 15, size=(10, 100))  # feature matrix\n",
    "coordinates = rng.uniform(0, 10, size=(10, 2))  # spatial coordinates\n",
    "image = rng.uniform(0, 1, size=(10, 10, 3))  # image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "from anndata import AnnData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = AnnData(counts, obsm={\"spatial\": coordinates})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb2e6a",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "I think this is similar to K-means but does not require the clusters to have a convex shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c89786",
   "metadata": {},
   "source": [
    "### scikitlearn \n",
    "https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3bcac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a58d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40a1642e",
   "metadata": {},
   "source": [
    "## Covariance of coordinates matrix\n",
    "The idea is that I will take the covariance of the matrix of 3D coordinates where the matrix is 3xN and N is the number of synapses. This will be for each cluster. Then I will shuffle the cluster labels and take the covariances of those coordinate matrices and compare. \n",
    "I'll work with the diagonal of the covariance matrix for now since that will just tell me how spread out things are within a dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f7415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "import random\n",
    "import numpy as np\n",
    "xs = []\n",
    "ys = []\n",
    "zs = []\n",
    "\n",
    "mu = 0\n",
    "sigma = 0.5\n",
    "samples = 100\n",
    "\n",
    "xs = np.random.normal(mu, sigma, samples)\n",
    "ys = np.random.normal(mu, sigma, samples)\n",
    "zs = np.random.normal(mu, sigma, samples)\n",
    "\n",
    "labels = np.ones(samples)\n",
    "#coords = np.vstack((xs, ys, zs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e30bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another cluster\n",
    "xss = []\n",
    "yss = []\n",
    "zss = []\n",
    "\n",
    "mu = 3\n",
    "sigma = 0.25\n",
    "samples = 100\n",
    "\n",
    "xss = np.random.normal(mu, sigma, samples)\n",
    "yss = np.random.normal(mu, sigma, samples)\n",
    "zss = np.random.normal(mu, sigma, samples)\n",
    "\n",
    "labelss = np.ones(samples) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78fdc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine clusters\n",
    "xs = np.concatenate((xs, xss))\n",
    "ys = np.concatenate((ys, yss))\n",
    "zs = np.concatenate((zs, zss))\n",
    "\n",
    "coords = np.vstack((xs, ys, zs))\n",
    "\n",
    "labels = np.concatenate((labels, labelss)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "# an array with all ones\n",
    "#labels = np.ones(samples)\n",
    "\n",
    "# randomly assign cluster labels\n",
    "#labels = np.random.randint(0, 2, 2*samples)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# color by label\n",
    "colors = ['r' if label == 1 else 'b' for label in labels]\n",
    "ax.scatter(xs, ys, zs, c=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2282804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab coordinates for a given cluster\n",
    "cluster_coords = coords[:, labels == 2]\n",
    "cluster_coords.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d527cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance matrix\n",
    "cov_matrix = np.cov(cluster_coords)\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace of covariance matrix\n",
    "np.trace(cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0789cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for covariance trace \n",
    "def calculate_spread(coords, labels):\n",
    "    \"\"\" \n",
    "    Calculate the weighted trace of covariance matrices for clusters. Your coords matrix should be 3xN where N is the number of samples. Labels is a 1D array of cluster labels for each sample.\n",
    "    \"\"\"\n",
    "    total_trace = 0\n",
    "    for label in np.unique(labels):\n",
    "        # grab the coordinates for the samples in a cluster\n",
    "        cluster_coords = coords[:, labels == label]\n",
    "\n",
    "        # covariance matrix and trace\n",
    "        cov_matrix = np.cov(cluster_coords)\n",
    "        trace = np.trace(cov_matrix)\n",
    "\n",
    "        # weight the trace by the cluster size and add to total spread\n",
    "        weight = len(labels[labels == label]) / len(labels)\n",
    "        total_trace += trace * weight\n",
    "\n",
    "    return total_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ee10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spread = calculate_spread(coords, labels)\n",
    "spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance # Useful for alternative distance metrics\n",
    "\n",
    "def calculate_spread_metric(coordinates, labels):\n",
    "    \"\"\"\n",
    "    Calculates the weighted average of the Trace of the Covariance Matrix\n",
    "    for each labeled cluster.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(labels)\n",
    "    total_trace = 0\n",
    "    total_synapses = len(coordinates)\n",
    "\n",
    "    if total_synapses == 0:\n",
    "        return 0\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # 1. Filter coordinates for the current label\n",
    "        cluster_coords = coordinates[labels == label]\n",
    "\n",
    "        # Ensure the cluster has enough points for covariance (at least 2)\n",
    "        if len(cluster_coords) < 2:\n",
    "            continue\n",
    "\n",
    "        # 2. Calculate the 3x3 Covariance Matrix (default is column-wise, so (3, N))\n",
    "        # Note: np.cov expects (features x observations). Our features are (x, y, z).\n",
    "        # We need to transpose the (N, 3) array to (3, N).\n",
    "        cov_matrix = np.cov(cluster_coords, rowvar=False) \n",
    "        \n",
    "        # 3. Calculate the Trace (sum of diagonal variances)\n",
    "        trace = np.trace(cov_matrix)\n",
    "        \n",
    "        # 4. Weight the trace by the cluster size\n",
    "        weight = len(cluster_coords) / total_synapses\n",
    "        total_trace += weight * trace\n",
    "\n",
    "    return total_trace\n",
    "\n",
    "def run_permutation_test(coords_3d, labels, num_permutations=1000):\n",
    "    \"\"\"\n",
    "    Performs the permutation test to assess if the observed spread is tighter\n",
    "    than a random distribution of labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Calculate the Observed Metric\n",
    "    # M_obs = Weighted Average of Tr(Covariance Matrix)\n",
    "    M_obs = calculate_spread_metric(coords_3d, labels)\n",
    "    print(f\"Observed Spread Metric (M_obs): {M_obs:.4f}\")\n",
    "    \n",
    "    # 2. Create Null Distribution\n",
    "    random_metrics = []\n",
    "    original_labels = np.copy(labels)\n",
    "\n",
    "    for i in range(num_permutations):\n",
    "        # Randomly shuffle the labels\n",
    "        np.random.shuffle(original_labels)\n",
    "        \n",
    "        # Calculate the metric for the shuffled configuration\n",
    "        M_random = calculate_spread_metric(coords_3d, original_labels)\n",
    "        random_metrics.append(M_random)\n",
    "\n",
    "    random_metrics = np.array(random_metrics)\n",
    "    \n",
    "    # 3. Statistical Test\n",
    "    # For clustering, we expect M_obs to be SMALLER than M_random\n",
    "    # The p-value is the proportion of random metrics that are <= M_obs\n",
    "    p_value = np.sum(random_metrics <= M_obs) / num_permutations\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Number of Permutations: {num_permutations}\")\n",
    "    print(f\"Mean Random Spread: {np.mean(random_metrics):.4f}\")\n",
    "    print(f\"P-value (Proportion of random spreads <= M_obs): {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"Conclusion: The observed clusters are significantly TIGHTER (more clustered) than expected by chance.\")\n",
    "    else:\n",
    "        print(\"Conclusion: The observed clustering is NOT significantly different from a random arrangement of labels.\")\n",
    "\n",
    "# --- Example Data Simulation ---\n",
    "\n",
    "# Simulate 2,000 synapses\n",
    "N_synapses = 2000\n",
    "\n",
    "# Create two genuinely clustered groups (Label A and Label B)\n",
    "# Label A is centered at (10, 10, 10) and Label B at (100, 100, 100)\n",
    "coords_A = np.random.normal(loc=[10, 10, 10], scale=5, size=(N_synapses//2, 3))\n",
    "labels_A = np.full(N_synapses//2, 'A')\n",
    "\n",
    "coords_B = np.random.normal(loc=[100, 100, 100], scale=5, size=(N_synapses//2, 3))\n",
    "labels_B = np.full(N_synapses//2, 'B')\n",
    "\n",
    "# Combine the data\n",
    "all_coords = np.vstack((coords_A, coords_B))\n",
    "all_labels = np.hstack((labels_A, labels_B))\n",
    "\n",
    "# --- Run the Test ---\n",
    "run_permutation_test(all_coords, all_labels, num_permutations=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hemibrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
